{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "3. Implement a program to train a binary logistic regression model using mini-batch SGD. Use the logistic regression model we derived in class, corresponding to Equation (4.90) from the textbook, and where the feature transformation $\\phi$ is the identity function.\n",
        "\n",
        "The program should include the following hyperparameters:\n",
        "- Batch size\n",
        "- Fixed learning rate\n",
        "- Maximum number of iterations"
      ],
      "metadata": {
        "id": "O5O207zVKvSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The logistic regression model we derived in class, corresponding to Equation (4.90) from the textbook, gives the cross-entropy loss:\n",
        " $$E(\\mathbf{w}) = -\\sum_{n=1}^N \\left\\{t_n \\log \\left(\\sigma(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_n))\\right) + (1 - t_n) \\log \\left(1 - \\sigma(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_n))\\right) \\right\\} = \\sum_{n=1}^N E_n(\\mathbf{w})$$\n",
        "\n",
        " $$\\Rightarrow \\hspace{0.07cm} E_n(\\mathbf{w}) = - t_n \\log \\left(\\sigma(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_n))\\right) - (1 - t_n) \\log \\left(1 - \\sigma(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_n))\\right)$$"
      ],
      "metadata": {
        "id": "Smeph5o1woi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing the gradient of $E_n(\\mathbf{w})$:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{d \\sigma}{d z}\n",
        "& = \\frac{d}{dz} (1 + \\exp(-z))^{-1} = -(1 + \\exp(-z))^{-2} (-\\exp(-z)) \\\\\n",
        "& = \\frac{\\exp(-z)}{(1 + \\exp(-z))^2} = \\frac{1}{1 + \\exp(-z)} \\left(\\frac{\\exp(-z)}{1 + \\exp(-z)}\\right) \\\\\n",
        "& = \\frac{1}{1 + \\exp(-z)} \\left(\\frac{1 + \\exp(-z)}{1 + \\exp(-z)} - \\frac{1}{1 + \\exp(-z)}\\right) \\\\\n",
        "& = \\frac{1}{1 + \\exp(-z)} \\left(1 - \\frac{1}{1 + \\exp(-z)}\\right) \\\\\n",
        "& = \\sigma(z) (1 - \\sigma(z))\n",
        "\\end{align*}\n",
        "\n",
        "\\begin{align*}\n",
        "\\Rightarrow \\nabla E_n(\\mathbf{w})\n",
        "& = - t_n \\left(\\frac{\\sigma(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_n)) \\left(1 - \\sigma(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_n))\\right) \\boldsymbol{\\phi}(\\mathbf{x}_n)}{\\sigma(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_n))}\\right) - (1 - t_n) \\left(\\frac{-\\sigma(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_n)) \\left(1 - \\sigma(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_n))\\right) \\boldsymbol{\\phi}(\\mathbf{x}_n)}{1 - \\sigma(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_n))}\\right) \\\\\n",
        "& = - t_n \\left(1 - \\sigma(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_n))\\right) \\boldsymbol{\\phi}(\\mathbf{x}_n) + (1 - t_n) \\sigma(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_n)) \\boldsymbol{\\phi}(\\mathbf{x}_n) \\\\\n",
        "& = \\left(-t_n + t_n \\sigma(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_n)) \\right) \\boldsymbol{\\phi}(\\mathbf{x}_n) + \\left(\\sigma(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_n)) - t_n \\sigma(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_n)) \\right) \\boldsymbol{\\phi}(\\mathbf{x}_n) \\\\\n",
        "& = \\left(-t_n + t_n \\sigma(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_n)) + \\sigma(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_n)) - t_n \\sigma(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_n)) \\right) \\boldsymbol{\\phi}(\\mathbf{x}_n) \\\\\n",
        "& = \\left(\\sigma(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_n)) - t_n \\right) \\boldsymbol{\\phi}(\\mathbf{x}_n)\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "AGJPYjx3wij8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic gradient descent (SGD) makes an update to the weight vector based on one data point at a time, so that\n",
        "\\begin{equation}\n",
        "\\mathbf{w}^{(\\tau+1)} = \\mathbf{w}^{(\\tau)} - \\eta \\nabla E_n (\\mathbf{w}^{(\\tau)}).\n",
        "\\tag*{(5.43)}\n",
        "\\end{equation}\n",
        "\n",
        "Thus, for mini-batch SGD with a fixed learning rate $\\eta$, the update rule is $$\\mathbf{w}^{(k+1)} = \\mathbf{w}^{(k)} - \\eta \\sum_{n \\in B_k} \\nabla E_n (\\mathbf{w}^{(k)}).$$"
      ],
      "metadata": {
        "id": "LVc1y7x9iCmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def log_sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def log_regression(x, t, initial_w, batch_size, learning_rate, max_iterations):\n",
        "  w = initial_w\n",
        "  for k in range(max_iterations):\n",
        "    # Randomly shuffle the samples or rows (without replacement)\n",
        "    indices = np.random.permutation(x.shape[0])\n",
        "    x = x[indices]\n",
        "    t = t[indices]\n",
        "    design_mat = np.hstack((np.ones((x.shape[0], 1)), x))\n",
        "\n",
        "    # Randomly select a batch\n",
        "    b_k = np.random.choice(x.shape[0], batch_size, replace=False)\n",
        "\n",
        "    # Compute the gradient\n",
        "    grad = 0\n",
        "    for n in b_k:\n",
        "      phi_n = design_mat[n]\n",
        "      grad += (log_sigmoid(np.dot(w, phi_n)) - t[n]) * phi_n\n",
        "\n",
        "    # Update w\n",
        "    w -= learning_rate * grad\n",
        "\n",
        "  return w\n"
      ],
      "metadata": {
        "id": "T1XaP92GK6Ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. In this problem, you will run a logistic regression model for classification on a breast cancer dataset."
      ],
      "metadata": {
        "id": "N4p5k_yQUzQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Download the Wisconsin Breast Cancer dataset from the UCI Machine Learning Repository or scikit-learnâ€™s built-in datasets."
      ],
      "metadata": {
        "id": "_mwIU1ztWMjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "\n",
        "# Load and return the breast cancer wisconsin dataset from scikit-learn\n",
        "breast_cancer = load_breast_cancer(as_frame=True)\n",
        "x = breast_cancer.data  # (data matrix)\n",
        "t = breast_cancer.target  # (classification target)"
      ],
      "metadata": {
        "id": "ovB5pYKAWyD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Split the dataset into train, validation, and test sets."
      ],
      "metadata": {
        "id": "DH_gAHOlZeAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Randomly split the data into train, validation, and test sets (75%-15%-10%)\n",
        "# Split x and t into 75% training and 25% validation/test\n",
        "x_train, x_val_test, t_train, t_val_test = train_test_split(x, t,\n",
        "                                                            test_size=0.25,\n",
        "                                                            random_state=1)\n",
        "# Split the validation/test sets into 60% validation and 40% test\n",
        "x_val, x_test, t_val, t_test = train_test_split(x_val_test, t_val_test,\n",
        "                                                test_size=0.4, random_state=1)"
      ],
      "metadata": {
        "id": "FXmQRjyP-UaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Report the size of each class in your training (+ validation) set."
      ],
      "metadata": {
        "id": "oFEa3yqIgVYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"Size of class 0 in training set:\", t_train.value_counts()[0],\n",
        "      \"\\nSize of class 1 in training set:\", t_train.value_counts()[1],\n",
        "      \"\\n\\nSize of class 0 in validation set:\", t_val.value_counts()[0],\n",
        "      \"\\nSize of class 1 in training set:\", t_val.value_counts()[1],\n",
        "      \"\\n\\nSize of class 0 in test set:\", t_test.value_counts()[0],\n",
        "      \"\\nSize of class 1 in test set:\", t_test.value_counts()[1])"
      ],
      "metadata": {
        "id": "92MGK6IjJZG4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e5b8cec-2535-4320-8e20-7e427f9d48f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of class 0 in training set: 157 \n",
            "Size of class 1 in training set: 269 \n",
            "\n",
            "Size of class 0 in validation set: 32 \n",
            "Size of class 1 in training set: 53 \n",
            "\n",
            "Size of class 0 in test set: 23 \n",
            "Size of class 1 in test set: 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) Train a binary logistic regression model using your implementation from problem 3. Initialize the model weights randomly, sampling from a standard Gaussian distribution. Experiment with different choices of fixed learning rate and batch size."
      ],
      "metadata": {
        "id": "KClSPKTScYe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Experimenting different choices of fixed learning rate and batch size\n",
        "## with the validation set to find the best choice:\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(1)\n",
        "\n",
        "# Initialize w randomly by sampling from a standard Gaussian distribution\n",
        "initial_w = np.random.normal(0, 1, x.shape[1] + 1)\n",
        "\n",
        "# Normalize all the features of x_train and x_val\n",
        "x_train = (x_train - np.mean(x_train, axis=0)) / np.std(x_train, axis=0)\n",
        "x_val = (x_val - np.mean(x_val, axis=0)) / np.std(x_val, axis=0)\n",
        "\n",
        "def evaluate_performance_on_validation_set(w_train, x_val, t_val):\n",
        "  x_val_design = np.hstack((np.ones((x_val.shape[0], 1)), x_val))\n",
        "  predicted_t_val = log_sigmoid(x_val_design @ w_train)\n",
        "  accuracy = np.mean(predicted_t_val == t_val)\n",
        "  return accuracy\n",
        "\n",
        "learning_rates = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
        "batch_sizes = [30, 60, 90, 120, 150]\n",
        "\n",
        "best_learning_rate = 0\n",
        "best_batch_size = 0\n",
        "best_accuracy = 0\n",
        "\n",
        "for learning_rate in learning_rates:\n",
        "  for batch_size in batch_sizes:\n",
        "    w_train = log_regression(x_train.values, t_train.values, initial_w.copy(),\n",
        "                             batch_size, learning_rate, max_iterations=100)\n",
        "    accuracy = evaluate_performance_on_validation_set(w_train, x_val.values, t_val.values)\n",
        "    if accuracy > best_accuracy:\n",
        "      best_accuracy = accuracy\n",
        "      best_learning_rate = learning_rate\n",
        "      best_batch_size = batch_size\n",
        "\n",
        "best_learning_rate, best_batch_size"
      ],
      "metadata": {
        "id": "46IfVHOFKV_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c778de16-a20b-4880-8b26-39f175d56a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.1, 120)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Using the best choice of fixed learning rate and batch size:\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(1)\n",
        "\n",
        "# Initialize w randomly by sampling from a standard Gaussian distribution\n",
        "initial_w = np.random.normal(0, 1, x.shape[1] + 1)\n",
        "\n",
        "# Normalize all the features of x_train and x_val\n",
        "x_train = (x_train - np.mean(x_train, axis=0)) / np.std(x_train, axis=0)\n",
        "\n",
        "# Train a binary logistic regression model by using my implementation from Problem 3\n",
        "w_train = log_regression(x_train.values, t_train.values, initial_w,\n",
        "                         best_batch_size, best_learning_rate, max_iterations=100)"
      ],
      "metadata": {
        "id": "zjtQ9zzzczNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(e) Use the trained model to report the performance of the model on the test set. For evaluation metrics, use accuracy, precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "gzcG2SffgUDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Normalize all the features of x_test\n",
        "x_test = (x_test - np.mean(x_test, axis=0)) / np.std(x_test, axis=0)\n",
        "\n",
        "# Use the trained model on the test set\n",
        "x_test_design = np.hstack((np.ones((x_test.shape[0], 1)), x_test))\n",
        "predicted_t_test = log_sigmoid(x_test_design @ w_train)\n",
        "\n",
        "# Convert to binary class labels\n",
        "predicted_t_test = (predicted_t_test >= 0.5).astype(int)\n",
        "\n",
        "# Report the model's performance\n",
        "print(\"Accuracy:\", accuracy_score(t_test, predicted_t_test),\n",
        "      \"\\nPrecision:\", precision_score(t_test, predicted_t_test),\n",
        "      \"\\nRecall:\", recall_score(t_test, predicted_t_test),\n",
        "      \"\\nF1-Score:\", f1_score(t_test, predicted_t_test))"
      ],
      "metadata": {
        "id": "6kHNt5ZeKaDb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3879ae26-fed4-43ee-a28b-b1ad23dded71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9827586206896551 \n",
            "Precision: 0.9722222222222222 \n",
            "Recall: 1.0 \n",
            "F1-Score: 0.9859154929577465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(f) Summarize your findings."
      ],
      "metadata": {
        "id": "mjkPVC0pgZfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I found that the trained binary logistic regression model performed very well on the test set of the Wisconsin Breast Cancer dataset based on all four evaluation metrics (i.e., accuracy, precision, recall, and F1-score). This dataset contains features that are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass and describe characteristics of the cell nuclei present in the image. The target variable, diagnosis, has two classes: malignant (M) and benign (B). For binary-class classification, we can assume that malignant is the positive condition and benign is the negative condition. Regarding evaluation metrics, the trained model's predictions for breast cancer diagnosis based on the test set's features were largely accurate. Specifically, an accuracy of approximately 98.3% indicates that the model correctly predicted almost all diagnoses. A precision of approximately 97.2% means that when predicting malignant diagnoses, the model was correct 97.2% of the time, and a recall of 100% indicates that the model correctly predicted all of the actual malignant diagnoses. Furthermore, an F1-score of approximately 98.6% shows both high precision and recall. Therefore, taking my results for evaluation metrics into account, the trained binary logistic regression model is reliable to make accurate predictions on unseen data as it demonstrated strong generalization to the test set."
      ],
      "metadata": {
        "id": "c55w1gvPKdaH"
      }
    }
  ]
}